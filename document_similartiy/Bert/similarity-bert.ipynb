{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U datasets\n!pip install transformers sentence-transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nprint(datasets.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:21.374223Z","iopub.execute_input":"2024-02-21T18:14:21.374600Z","iopub.status.idle":"2024-02-21T18:14:22.578786Z","shell.execute_reply.started":"2024-02-21T18:14:21.374571Z","shell.execute_reply":"2024-02-21T18:14:22.577837Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.17.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, models\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport time\nimport datetime\nimport random\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:24.024604Z","iopub.execute_input":"2024-02-21T18:14:24.025144Z","iopub.status.idle":"2024-02-21T18:14:30.317940Z","shell.execute_reply.started":"2024-02-21T18:14:24.025112Z","shell.execute_reply":"2024-02-21T18:14:30.317116Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:30.319804Z","iopub.execute_input":"2024-02-21T18:14:30.320801Z","iopub.status.idle":"2024-02-21T18:14:30.377769Z","shell.execute_reply.started":"2024-02-21T18:14:30.320763Z","shell.execute_reply":"2024-02-21T18:14:30.376895Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(\"stsb_multi_mt\", \"en\")\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:30.378723Z","iopub.execute_input":"2024-02-21T18:14:30.378981Z","iopub.status.idle":"2024-02-21T18:14:37.374388Z","shell.execute_reply.started":"2024-02-21T18:14:30.378958Z","shell.execute_reply":"2024-02-21T18:14:37.373495Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/11.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44dcf61f26064facbf167ad43448734d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/470k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c2a06707aaf4b939dee5fb2838a0813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/108k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0882c424117498e8c4e3f862037ae24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/142k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639be4c8a63242268f5c245fb609b77e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782d745d570f4650b122df2c49e78a71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b06e11fa428c416484a6877ffba89b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9b39cef86f485c9fe149422eb2d87f"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 5749\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 1379\n    })\n    dev: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 1500\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"A sample from the STSB dataset's training split:\")\nprint(dataset['train'][3])","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:37.376374Z","iopub.execute_input":"2024-02-21T18:14:37.376709Z","iopub.status.idle":"2024-02-21T18:14:37.381757Z","shell.execute_reply.started":"2024-02-21T18:14:37.376681Z","shell.execute_reply":"2024-02-21T18:14:37.380696Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"A sample from the STSB dataset's training split:\n{'sentence1': 'Three men are playing chess.', 'sentence2': 'Two men are playing chess.', 'similarity_score': 2.5999999046325684}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:37.382984Z","iopub.execute_input":"2024-02-21T18:14:37.383292Z","iopub.status.idle":"2024-02-21T18:14:38.655598Z","shell.execute_reply.started":"2024-02-21T18:14:37.383260Z","shell.execute_reply":"2024-02-21T18:14:38.654793Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0520d89b3f4c7ea304922e430c1035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cccb465a1eb74dcfacee44b8fea43ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c74f67b07f6479798186cac919d4792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0fdc32b8c04ab3a84d57bafb6df43d"}},"metadata":{}}]},{"cell_type":"code","source":"class STSBDataset(torch.utils.data.Dataset):\n\n    def __init__(self, dataset):\n        # Normalize the similarity scores in the dataset\n        similarity_scores = [i['similarity_score'] for i in dataset]\n        self.normalized_similarity_scores = [i/5.0 for i in similarity_scores]\n        self.first_sentences = [i['sentence1'] for i in dataset]\n        self.second_sentences = [i['sentence2'] for i in dataset]\n        self.concatenated_sentences = [[str(x), str(y)] for x,y in   zip(self.first_sentences, self.second_sentences)]\n\n    def __len__(self):\n        return len(self.concatenated_sentences)\n\n    def get_batch_labels(self, idx):\n        return torch.tensor(self.normalized_similarity_scores[idx])\n\n    def get_batch_texts(self, idx):\n        return tokenizer(self.concatenated_sentences[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n        return batch_texts, batch_y\n\n\ndef collate_fn(texts):\n    input_ids = texts['input_ids']\n    attention_masks = texts['attention_mask']\n    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n                for input_id, attention_mask in zip(input_ids, attention_masks)]\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:38.656751Z","iopub.execute_input":"2024-02-21T18:14:38.657045Z","iopub.status.idle":"2024-02-21T18:14:38.666957Z","shell.execute_reply.started":"2024-02-21T18:14:38.657019Z","shell.execute_reply":"2024-02-21T18:14:38.666029Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class BertForSTS(torch.nn.Module):\n\n    def __init__(self):\n        super(BertForSTS, self).__init__()\n        self.bert = models.Transformer('bert-base-uncased', max_seq_length=128)\n        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n\n    def forward(self, input_data):\n        output = self.sts_bert(input_data)['sentence_embedding']\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:38.668156Z","iopub.execute_input":"2024-02-21T18:14:38.668840Z","iopub.status.idle":"2024-02-21T18:14:38.682258Z","shell.execute_reply.started":"2024-02-21T18:14:38.668777Z","shell.execute_reply":"2024-02-21T18:14:38.681387Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = BertForSTS()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:38.683409Z","iopub.execute_input":"2024-02-21T18:14:38.683784Z","iopub.status.idle":"2024-02-21T18:14:42.350176Z","shell.execute_reply.started":"2024-02-21T18:14:38.683742Z","shell.execute_reply":"2024-02-21T18:14:42.349172Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b9f266ce1e546c3bf7c25b05d270ba6"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BertForSTS(\n  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n  (sts_bert): SentenceTransformer(\n    (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"class CosineSimilarityLoss(torch.nn.Module):\n\n    def __init__(self,  loss_fn=torch.nn.MSELoss(), transform_fn=torch.nn.Identity()):\n        super(CosineSimilarityLoss, self).__init__()\n        self.loss_fn = loss_fn\n        self.transform_fn = transform_fn\n        self.cos_similarity = torch.nn.CosineSimilarity(dim=1)\n\n    def forward(self, inputs, labels):\n        emb_1 = torch.stack([inp[0] for inp in inputs])\n        emb_2 = torch.stack([inp[1] for inp in inputs])\n        outputs = self.transform_fn(self.cos_similarity(emb_1, emb_2))\n        return self.loss_fn(outputs, labels.squeeze())","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:42.351512Z","iopub.execute_input":"2024-02-21T18:14:42.352429Z","iopub.status.idle":"2024-02-21T18:14:42.359774Z","shell.execute_reply.started":"2024-02-21T18:14:42.352392Z","shell.execute_reply":"2024-02-21T18:14:42.358804Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_ds = STSBDataset(dataset['train'])\nval_ds = STSBDataset(dataset['dev'])\n\n# Create a 90-10 train-validation split.\ntrain_size = len(train_ds)\nval_size = len(val_ds)\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:42.362492Z","iopub.execute_input":"2024-02-21T18:14:42.362766Z","iopub.status.idle":"2024-02-21T18:14:43.366064Z","shell.execute_reply.started":"2024-02-21T18:14:42.362743Z","shell.execute_reply":"2024-02-21T18:14:43.365110Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"5,749 training samples\n1,500 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 8\n\ntrain_dataloader = DataLoader(\n            train_ds,  # The training samples.\n            num_workers = 4,\n            batch_size = batch_size, # Use this batch size.\n            shuffle=True # Select samples randomly for each batch\n        )\n\nvalidation_dataloader = DataLoader(\n            val_ds,\n            num_workers = 4,\n            batch_size = batch_size # Use the same batch size\n        )","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:43.367335Z","iopub.execute_input":"2024-02-21T18:14:43.367702Z","iopub.status.idle":"2024-02-21T18:14:43.373515Z","shell.execute_reply.started":"2024-02-21T18:14:43.367669Z","shell.execute_reply":"2024-02-21T18:14:43.372577Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 1e-6)\nepochs = 8\n# Total number of training steps is [number of batches] x [number of epochs].\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:14:43.374710Z","iopub.execute_input":"2024-02-21T18:14:43.374973Z","iopub.status.idle":"2024-02-21T18:14:43.387310Z","shell.execute_reply.started":"2024-02-21T18:14:43.374950Z","shell.execute_reply":"2024-02-21T18:14:43.386490Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train():\n  seed_val = 42\n  criterion = CosineSimilarityLoss()\n  criterion = criterion.cuda()\n  random.seed(seed_val)\n  torch.manual_seed(seed_val)\n  # We'll store a number of quantities such as training and validation loss,\n  # validation accuracy, and timings.\n  training_stats = []\n  for epoch_i in range(0, epochs):\n      total_train_loss = 0\n      model.train()\n      # For each batch of training data...\n      for train_data, train_label in tqdm(train_dataloader):\n          train_data['input_ids'] = train_data['input_ids'].to(device)\n          train_data['attention_mask'] = train_data['attention_mask'].to(device)\n          train_data = collate_fn(train_data)\n          model.zero_grad()\n          output = [model(feature) for feature in train_data]\n          loss = criterion(output, train_label.to(device))\n          total_train_loss += loss.item()\n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n          optimizer.step()\n          scheduler.step()\n\n      # Calculate the average loss over all of the batches.\n      avg_train_loss = total_train_loss / len(train_dataloader)            \n      model.eval()\n      total_eval_accuracy = 0\n      total_eval_loss = 0\n      nb_eval_steps = 0\n      for val_data, val_label in tqdm(validation_dataloader):\n          val_data['input_ids'] = val_data['input_ids'].to(device)\n          val_data['attention_mask'] = val_data['attention_mask'].to(device)\n          val_data = collate_fn(val_data)\n          with torch.no_grad():        \n              output = [model(feature) for feature in val_data]\n          loss = criterion(output, val_label.to(device))\n          total_eval_loss += loss.item()\n      avg_val_loss = total_eval_loss / len(validation_dataloader)\n      training_stats.append(\n          {\n              'epoch': epoch_i + 1,\n              'Training Loss': avg_train_loss,\n              'Valid. Loss': avg_val_loss,\n          }\n      )\n  return model, training_stats\n\n# Launch the training\nmodel, training_stats = train()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:33:50.083731Z","iopub.execute_input":"2024-02-21T18:33:50.084392Z","iopub.status.idle":"2024-02-21T19:06:15.245718Z","shell.execute_reply.started":"2024-02-21T18:33:50.084360Z","shell.execute_reply":"2024-02-21T19:06:15.244594Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 719/719 [03:44<00:00,  3.21it/s]\n100%|██████████| 188/188 [00:18<00:00, 10.01it/s]\n100%|██████████| 719/719 [03:44<00:00,  3.21it/s]\n100%|██████████| 188/188 [00:18<00:00,  9.97it/s]\n100%|██████████| 719/719 [03:43<00:00,  3.22it/s]\n100%|██████████| 188/188 [00:18<00:00,  9.95it/s]\n100%|██████████| 719/719 [03:43<00:00,  3.22it/s]\n100%|██████████| 188/188 [00:19<00:00,  9.87it/s]\n100%|██████████| 719/719 [03:45<00:00,  3.19it/s]\n100%|██████████| 188/188 [00:18<00:00,  9.90it/s]\n100%|██████████| 719/719 [03:44<00:00,  3.20it/s]\n100%|██████████| 188/188 [00:18<00:00,  9.95it/s]\n100%|██████████| 719/719 [03:44<00:00,  3.21it/s]\n100%|██████████| 188/188 [00:18<00:00,  9.89it/s]\n100%|██████████| 719/719 [03:44<00:00,  3.21it/s]\n100%|██████████| 188/188 [00:19<00:00,  9.89it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a DataFrame from our training statistics\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table\ndf_stats","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:32:52.290329Z","iopub.execute_input":"2024-02-21T18:32:52.291046Z","iopub.status.idle":"2024-02-21T18:32:52.303295Z","shell.execute_reply.started":"2024-02-21T18:32:52.291015Z","shell.execute_reply":"2024-02-21T18:32:52.302482Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"       Training Loss  Valid. Loss\nepoch                            \n1           0.026849     0.037488","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.026849</td>\n      <td>0.037488</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# load the test set\ntest_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"test\")\n\n# Prepare the data\nfirst_sent = [i['sentence1'] for i in test_dataset]\nsecond_sent = [i['sentence2'] for i in test_dataset]\nfull_text = [[str(x), str(y)] for x,y in zip(first_sent, second_sent)]\n\nmodel.eval()\n\ndef predict_similarity(sentence_pair):\n  test_input = tokenizer(sentence_pair, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\").to(device)\n  test_input['input_ids'] = test_input['input_ids']\n  test_input['attention_mask'] = test_input['attention_mask']\n  del test_input['token_type_ids']\n  output = model(test_input)\n  sim = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n  return sim","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:27:06.780819Z","iopub.execute_input":"2024-02-21T18:27:06.781177Z","iopub.status.idle":"2024-02-21T18:27:11.008690Z","shell.execute_reply.started":"2024-02-21T18:27:06.781146Z","shell.execute_reply":"2024-02-21T18:27:11.007925Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"example_1 = full_text[100]\nprint(f\"Sentence 1: {example_1[0]}\")\nprint(f\"Sentence 2: {example_1[1]}\")\nprint(f\"Predicted similarity score: {round(predict_similarity(example_1), 2)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-21T18:27:11.010375Z","iopub.execute_input":"2024-02-21T18:27:11.010811Z","iopub.status.idle":"2024-02-21T18:27:11.034064Z","shell.execute_reply.started":"2024-02-21T18:27:11.010777Z","shell.execute_reply":"2024-02-21T18:27:11.033181Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Sentence 1: A cat is walking around a house.\nSentence 2: A woman is peeling potato.\nPredicted similarity score: 0.03\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Sentence 1: {example_1[0]}\")\nprint(f\"Sentence 2: {example_1[1]}\")\nprint(f\"Predicted similarity score: {round(predict_similarity(example_1), 2)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/working/model_with_BERT'\ntorch.save(model.state_dict(), PATH)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyPDF2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_text(pdf_path):\n    with open(pdf_path, 'rb') as file:\n        reader = PyPDF2.PdfReader(file)\n        num_pages = len(reader.pages)\n        text = \"\"\n\n        for i in range(num_pages):\n            page = reader.pages[i]\n            text += page.extract_text() + \"\\n\"\n\n        return text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_abstract_to_references(text):\n    # Özet bölümünün başlangıcını ve kaynakça bölümünün sonunu bulmak için regüler ifadeler kullan.\n    abstract_start_regex = r\"(Abstract|Özet)\"\n    references_end_regex = r\"(References|Kaynakça)\"\n\n    # Metinde özetin başlangıcını ve kaynakçanın sonunu bul.\n    abstract_start_match = re.search(abstract_start_regex, text, re.IGNORECASE)\n    references_end_match = re.search(references_end_regex, text, re.IGNORECASE)\n\n    if abstract_start_match and references_end_match:\n        start_index = abstract_start_match.start()\n        end_index = references_end_match.end()\n        return text[start_index:end_index]\n    else:\n        return \"Abstract veya References bulunamadı.\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PDF dosya yolunu buraya girin\npdf_path = '/kaggle/input/artcle/Performance_analysis_of_TCP_incast_with_TCP_Lite_and_Abstract_TCP.pdf'  # Örnek: '/kaggle/input/artcle/Performance_analysis_of_TCP_incast_with_TCP_Lite_and_Abstract_TCP.pdf'\npdf_path2 = '/kaggle/input/artcle/TCP_Fairness_Among_Modern_TCP_Congestion_Control_Algorithms_Including_TCP_BBR.pdf'\npdf_path3 = '/kaggle/input/game-theo/A_Speed_Guide_Model_for_Collision_Avoidance_in_Non-Signalized_Intersections_Based_on_Reduplicate_Game_Theory.pdf'\npdf_path4 = '/kaggle/input/oyun-teo2/Device_to_Device_Communication_using_Stackelberg_Game_Theory_approach.pdf'\npdf_path5 = '/kaggle/input/haaaaa/Resource_Allocation_for_Device-to-Device_Communications_Underlaying_Heterogeneous_Cellular_Networks_Using_Coalitional_Games.pdf'  # Örnek: '/kaggle/input/artcle/Performance_analysis_of_TCP_incast_with_TCP_Lite_and_Abstract_TCP.pdf'\n\n\nfull_text_tcp = extract_text(pdf_path)\nfull_text_tcp2 = extract_text(pdf_path2)\nfull_text_GT = extract_text(pdf_path3)\nfull_text_GT2 = extract_text(pdf_path4)\nfull_text_GT3 = extract_text(pdf_path5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Abstract ile References arasındaki metni çıkar\nabstract_to_references_tcp = find_abstract_to_references(full_text_tcp)\nabstract_to_references_tcp2 = find_abstract_to_references(full_text_tcp2)\nabstract_to_references_GT = find_abstract_to_references(full_text_GT)\nabstract_to_references_GT2 = find_abstract_to_references(full_text_GT2)\nabstract_to_references_GT3 = find_abstract_to_references(full_text_GT3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = [abstract_to_references_tcp,abstract_to_references_tcp2]\nsentences2 = [abstract_to_references_tcp,abstract_to_references_GT]\nsentences3 = [abstract_to_references_tcp2,abstract_to_references_GT]\nsentences4 = [abstract_to_references_tcp,abstract_to_references_GT2]\nsentences5 = [abstract_to_references_tcp2,abstract_to_references_GT2]\nsentences6 = [abstract_to_references_GT2,abstract_to_references_GT]\nsentences7 = [abstract_to_references_GT2,abstract_to_references_GT3]\nsentences8 = [abstract_to_references_GT,abstract_to_references_GT3]\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\ndef calculate_similarity(sentences):\n    embeddings = model.encode(sentences)\n    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n    return similarity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity = calculate_similarity(sentences)\nsimilarity2 = calculate_similarity(sentences2)\nsimilarity3 = calculate_similarity(sentences3)\nsimilarity4 = calculate_similarity(sentences4)\nsimilarity5 = calculate_similarity(sentences5)\nsimilarity6 = calculate_similarity(sentences6)\nsimilarity7 = calculate_similarity(sentences7)\nsimilarity8 = calculate_similarity(sentences8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}