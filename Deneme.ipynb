{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb54fca-9fc5-4629-809b-f85f02c14c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Dosyayı aç ve pickle.dump ile nesneyi yaz\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Dosyayı aç ve pickle.dump ile nesneyi yaz\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60715b42-f319-483d-9547-d95e60bec52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\berk_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Eğer henüz üstteki modeller yüklenmediyse bunu yukarı kaydırıp onları yükle\n",
    "\"\"\"\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline, AutoTokenizer, BartForConditionalGeneration\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a776fa5-c4dc-41ad-8efe-f422c1f7f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Dosyayı aç ve pickle.load ile nesneyi oku\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d35192f-04a0-4ec1-9519-d1e08174ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sim yüklendi: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from Modules.load_models import load_model_similarty\n",
    "load_model_similarty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb3ebbd-e07b-4566-8b5b-90e3f231a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Text Segmentation by Cross Segment Attention \n",
      "girdi3 <LTTextBoxHorizontal(7) 72.000,395.196,77.978,407.152 '1\\n'> <LTTextBoxHorizontal(8) 89.933,395.196,154.814,407.152 'Introduction\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(10) 307.276,765.624,414.562,777.580 '2 Literature review\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(2) 72.000,508.690,159.213,520.646 '3 Architectures\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(5) 72.000,298.618,196.811,309.527 '3.1 Cross-segment BERT\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(8) 307.276,439.551,412.167,450.460 '3.2 BERT+Bi-LSTM\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(6) 72.000,369.983,187.124,380.892 '3.3 Hierarchical BERT\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(17) 307.276,357.029,449.639,368.985 '4 Evaluation methodology\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(18) 307.276,335.082,371.203,345.991 '4.1 Datasets\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(24) 307.276,589.912,367.560,600.821 '4.2 Metrics\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(27) 307.276,274.585,362.401,286.541 '5 Results\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(1) 72.000,699.299,135.100,711.255 '6 Analyses\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(8) 72.000,258.358,223.189,269.267 '6.1 Role of trailing context size\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(12) 307.276,205.282,486.360,216.191 '6.2 Role of Transformer architecture\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(8) 72.000,352.994,178.680,363.903 '6.3 Model distillation\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(18) 307.276,463.219,382.343,475.175 '7 Conclusion\\n'>\n",
      "References is detected.\n",
      "Title Attention Is All You Need \n",
      "girdi3 <LTTextBoxHorizontal(0) 108.000,707.538,113.978,719.494 '1\\n'> <LTTextBoxHorizontal(1) 125.933,707.538,190.814,719.494 'Introduction\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(6) 108.000,417.225,188.829,429.181 '2 Background\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(11) 108.000,137.821,226.093,149.777 '3 Model Architecture\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(2) 108.000,299.280,253.006,309.242 '3.1 Encoder and Decoder Stacks\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(5) 108.000,100.805,170.814,110.767 '3.2 Attention\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(4) 108.000,431.215,263.885,441.177 '3.2.1 Scaled Dot-Product Attention\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(17) 108.000,151.491,230.590,161.453 '3.2.2 Multi-Head Attention\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(12) 108.000,501.999,302.858,511.961 '3.2.3 Applications of Attention in our Model\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(17) 108.000,284.911,292.866,294.873 '3.3 Position-wise Feed-Forward Networks\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(22) 108.000,133.953,240.024,143.915 '3.4 Embeddings and Softmax\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(7) 108.000,568.481,215.198,578.443 '3.5 Positional Encoding\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(13) 108.000,283.438,225.041,295.394 '4 Why Self-Attention\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(3) 108.000,480.411,170.227,492.367 '5 Training\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(5) 108.000,429.544,249.529,439.506 '5.1 Training Data and Batching\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(7) 108.000,316.408,233.041,326.370 '5.2 Hardware and Schedule\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(9) 108.000,225.089,174.132,235.051 '5.3 Optimizer\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(15) 108.000,90.821,193.509,100.783 '5.4 Regularization\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(17) 108.000,409.039,163.125,420.995 '6 Results\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(18) 108.000,385.004,219.073,394.966 '6.1 Machine Translation\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(23) 108.000,124.646,203.940,134.608 '6.2 Model Variations\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(42) 108.000,221.781,255.974,231.743 '6.3 English Constituency Parsing\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(9) 108.000,415.504,183.067,427.460 '7 Conclusion\\n'>\n",
      "References is detected.\n"
     ]
    }
   ],
   "source": [
    "from Modules.article_tree import ArticleTree\n",
    "from Modules.reader import get_tree_from_article_pdf\n",
    "\n",
    "pdf_path2 = \"1706.03762.pdf\"\n",
    "pdf_path = \"NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf\"\n",
    "pdf_path = \"document_similartiy\\\\article\\\\TCP_Fairness_Among_Modern_TCP_Congestion_Control_Algorithms_Including_TCP_BBR.pdf\"\n",
    "pdf_path = \"A High-Performance Deep Neural Network Model for BI-RADS Classification of Screening Mammography.pdf\"\n",
    "pdf_path = \"200414535.pdf\"\n",
    "#pdf_path = \"k1.pdf\"\n",
    "tree = get_tree_from_article_pdf(pdf_path)\n",
    "tree2 = get_tree_from_article_pdf(pdf_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5deb7c49-62c0-4fad-a58e-5da80afd8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text Segmentation by Cross Segment Attention \n",
      ".   Introduction \n",
      "..   Text segmentation is a traditional NLP task that breaks up text into constituents, according to predeﬁned requirements. It can be applied to documents, in which case the objective is to create logically coherent sub-document units. These units, or segments, can be any structure of interest, such as paragraphs or sections. This task is often referred to as document segmentation or sometimes simply text segmentation. In Figure 1 we show one example of document segmentation from Wikipedia, on which the task is typically evaluated (Koshorek et al., 2018; Badjatiya et al., 2018). \n",
      "..   Documents are often multi-modal, in that they cover multiple aspects and topics; breaking a document into uni-modal segments can help improve and/or speed up down stream applications. For example, document segmentation has been shown to improve information retrieval by indexing subdocument units instead of full documents (Llopis et al., 2002; Shtekh et al., 2018). Other applications such as summarization and information extraction can also beneﬁt from text segmentation (Koshorek et al., 2018). \n",
      "..   Early life and marriage: Franklin Delano Roosevelt was born on January 30, 1882, in the Hudson Valley town of Hyde Park, New York, to businessman James Roosevelt I and his second wife, Sara Ann Delano. (...) Aides began to refer to her at the time as “the president’s girlfriend”, and gossip linking the two romantically appeared in the newspapers. \n",
      "..   (...) \n",
      "..   Legacy: Roosevelt is widely considered to be one of the most important ﬁgures in the history of the United States, as well as one of the most inﬂuential ﬁgures of the 20th century. (...) Roosevelt has also appeared on several U.S. Postage stamps. \n",
      "..   Figure 1: Illustration of text segmentation on the example of the Wikipedia page of President Roosevelt. The aim of document segmentation is breaking the raw text into a sequence of logically coherent sections (e.g., “Early life and marriage” and “Legacy” in our example). \n",
      "..   A related task called discourse segmentation breaks up pieces of text into sub-sentence elements called Elementary Discourse Units (EDUs). EDUs are the minimal units in discourse analysis according to the Rhetorical Structure Theory (Mann and Thompson, 1988). In Figure 2 we show examples of EDU segmentations of sentences. For example, the sentence “Annuities are rarely a good idea at the age 35 because of withdrawal restrictions” decomposes into the following two EDUs: “Annuities are rarely a good idea at the age 35” and “because of withdrawal restrictions”, the ﬁrst one being a statement and the second one being a justiﬁcation in the discourse analysis. In addition to being a key step in discourse analysis (Joty et al., 2019), discourse segmentation has been shown to improve a number of downstream tasks, such as text summarization, by helping to identify ﬁne-grained sub-sentence units that may have different levels of importance when creating a summary (Li et al., 2016). \n",
      "..   Multiple neural approaches have been recently proposed for document and discourse segmentation. Koshorek et al. (2018) proposed the use of \n",
      "..   Sentence 1: Annuities are rarely a good idea at the age 35 (cid:107) because of withdrawal restrictions Sentence 2: Wanted: (cid:107) An investment (cid:107) that’s as simple and secure as a certiﬁcate of deposit (cid:107) but offers a return (cid:107) worth getting excited about. \n",
      "..   Figure 2: Example discourse segmentations from the RST-DT dataset (Carlson et al., 2001). In the segmentations, the EDUs are separated by the (cid:107) character. \n",
      "..   hierarchical Bi-LSTMs for document segmentation. Simultaneously, Li et al. (2018) introduced an attention-based model for both document segmentation and discourse segmentation, and Wang et al. (2018) obtained state of the art results on discourse segmentation using pretrained contextual embeddings (Peters et al., 2018). Also, a new largescale dataset for document segmentation based on Wikipedia was introduced by Koshorek et al. (2018), providing a much more realistic setup for evaluation than the previously used small scale and often synthetic datasets such as the Choi dataset (Choi, 2000). \n",
      "..   However, these approaches are evaluated on different datasets and as such have not been compared against one another. Furthermore they mostly rely on RNNs instead of the more recent transformers (Vaswani et al., 2017) and in most cases do not make use of contextual embeddings which have been shown to help in many classical NLP tasks (Devlin et al., 2018). \n",
      "..   In this work we aim at addressing these limita\n",
      "..   tions and bring the following contributions: \n",
      "..   1. We compare recent approaches that were proposed independently for text and/or discourse segmentation (Li et al., 2018; Koshorek et al., 2018; Wang et al., 2018) on three public datasets. \n",
      "..   2. We introduce three new model architectures based on transformers and BERT-style contextual embeddings to the document and discourse segmentation tasks. We analyze the strengths and weaknesses of each architecture and establish a new state-of-the-art. \n",
      "..   3. We show that a simple paradigm argued for by some of the earliest text segmentation algorithms can achieve competitive performance in the current neural era. \n",
      "..   4. We conduct ablation studies analyzing the importance of context size and model size. \n",
      ".   Literature review \n",
      "..   Document segmentation Many early research efforts were focused on unsupervised text segmentation, doing so by quantifying lexical cohesion within small text segments (Hearst, 1997; Choi, 2000). Being hard to precisely deﬁne and quantify, lexical cohesion has often been approximated by counting word repetitions. Although computationally expensive, unsupervised Bayesian approaches have also been popular (Utiyama and Isahara, 2001; Eisenstein, 2009; Mota et al., 2019). However, unsupervised algorithms suffer from two main drawbacks: they are hard to specialize for a given domain and in most cases do not naturally deal with multi-scale issues. Indeed, the desired segmentation granularity (paragraph, section, chapter, etc.) is necessarily task dependent and supervised learning provides a way of addressing this property. Therefore, supervised algorithms have been a focus of many recent works. \n",
      "..   In particular, multiple neural approaches have been proposed for the task. In one, a sequence labeling algorithm is proposed where each sentence is encoded using a Bi-LSTM over tokens, and then a Bi-LSTM over sentence encodings is used to label each sentence as ending a segment or not (Koshorek et al., 2018). Authors consider a large dataset based on Wikipedia, and report improvements over unsupervised text segmentation methods. In another work, a sequence-to-sequence model is proposed (Li et al., 2018), where the input is encoded using a BiGRU and segment endings are generated using a pointer network (Vinyals et al., 2015). The authors report signiﬁcant improvements over sequence labeling approaches, however on a dataset composed of 700 artiﬁcial documents created by concatenating segments from random articles from the Brown corpus (Choi, 2000). Lastly, Badjatiya et al. (2018) consider an attention-based CNN-Bi-LSTM model and evaluate it on three small-scale datasets. \n",
      "..   Discourse Segmentation Contrary to document segmentation, discourse segmentation has historically been framed as a supervised learning task. However, a challenge of applying supervised approaches for this type of segmentation is the fact that the available dataset for the task is limited (Carlson et al., 2001). For this reason, approaches for discourse segmentation usually rely on external annotations and resources to help the models generalize. Early approaches to discourse segmen\n",
      "..   tation were based on features from linguistic annotations such as POS tags and parsing trees (Soricut and Marcu, 2003; Xuan Bach et al., 2012; Joty et al., 2015). The performance of these systems was highly dependent on the quality of the annotations. \n",
      "..   Recent approaches started to rely on end-to-end neural network models that do not need linguistic annotations to obtain high-quality results, relying instead on pretrained models to obtain word or sentence representations. An example of such work is by Li et al. (2018), which proposes a sequenceto-sequence model getting a sequence of GloVe (Pennington et al., 2014) word embeddings as input and generating the EDU breaks. Another approach utilizes ELMO pretrained embeddings in the CRFBi-LSTM architecture and achieves state-of-the-art results on the task (Wang et al., 2018). \n",
      ".   Architectures \n",
      "..   We propose three model architectures for segmentation. One uses only local context around each candidate break, while the other two leverage the full context from the input (by candidate break, we mean any potential segment boundary). \n",
      "..   All our models rely on the same preprocessing technique and simply feed the raw input into a word-piece (sub-word) tokenizer (Wu et al., 2016). We use the word-piece tokenizer implementation that was open-sourced as part of the BERT release (Devlin et al., 2018), more precisely its English, uncased variant, which has a vocabulary size of 30,522 word-pieces. \n",
      "..   Cross-segment BERT \n",
      "...   For our ﬁrst model, we represent each candidate break by its left and right local contexts, i.e., the sequences of word-piece tokens that come before and after, respectively, the candidate break. The main motivation for this model is its simplicity; however, using only local contexts might be sub-optimal, as longer distance linguistic artifacts are likely to help locating breaks. Using such a simple model is a departure from recent trends favoring hierarchical models, which are conceptually appealing to model documents. However, it is also interesting to note that using local context was a common approach with earlier text segmentation models, such as (Hearst, 1997), which were studying semantic shift by comparing the word distributions before and after each candidate break. \n",
      "...   In Figure 3(a) we illustrate the model. The input is composed of a [CLS] token, followed by the two contexts concatenated together, and separated by a [SEP] token. When necessary, short contexts are padded to the left or to the right with [PAD] tokens. [CLS], [SEP] and [PAD] are special tokens introduced by BERT (Devlin et al., 2018). They stand for, respectively, ”classiﬁcation token” (since it is typically for classiﬁcation tasks, as a representation of the entire input sequence), ”separator token” and ”padding token”. The input is then fed into a transformer encoder (Vaswani et al., 2017), which is initialized with the publicly available BERTLARGE model. The BERTLARGE model has 24 layers, uses 1024-dimensional embeddings and 16 attention heads. The model is then ﬁne-tuned on each task. The released BERT checkpoint supports sequences of up to 512 tokens, so we keep at most 255 word-pieces for each side. We study the effect of length of the contexts, and denote the context conﬁguration by n-m where n and m are the number of word piece tokens before and after the [SEP] token. \n",
      "..   BERT+Bi-LSTM \n",
      "...   Our second proposed model is illustrated in Figure 3(b). It starts by encoding each sentence with BERTLARGE independently. Then, the tensors produced for each sentence are fed into a Bi-LSTM that is responsible for capturing a representation of the sequence of sentences with an indeﬁnite size. When encoding each sentence with BERT, all the sequences start with a [CLS] token. If the segmentation decision is made at the sentence level (e.g., document segmentation), we use the [CLS] token as input of the LSTM. In cases in which the segmentation decision is made at the word level (e.g., discourse segmentation), we obtain BERT’s full sequence output and use the left-most wordpiece of each word as an input to LSTM. Note that, due to the context being short for the discourse segmentation task, it is fully encoded in a single pass using BERT. Alternatively, one could encode each word independently; considering that many words consist of a single word-piece, encoding them with a deep transformer encoder would be somewhat wasteful of computing resources. \n",
      "...   With this model, we reduce the BERT’s inputs to a maximum sentence size of 64 tokens. Keeping this size small helps reduce training and inference times, since the computational cost of transformers \n",
      "...   (a) Cross-Segment BERT \n",
      "...   (b) BERT+Bi-LSTM \n",
      "...   (c) Hierarchical BERT \n",
      "...   Figure 3: Our proposed segmentation models, illustrating the document segmentation task. In the cross-segment BERT model (left), we feed a model with a local context surrounding a potential segment break: k tokens to the left and k tokens to the right. In the BERT+Bi-LSTM model (center) we ﬁrst encode each sentence using a BERT model, and then feed the sentence representations into a Bi-LSTM. In the hierarchical BERT model (right), we ﬁrst encode each sentence using BERT and then feed the output sentence representations in another transformer-based model. \n",
      "...   (and self-attention in particular) increases quadratically with the input length. Then, the LSTM is responsible for handling the diverse and potentially large sequence of sentences with linear computational complexity. In practice, we set a maximum document length of 128 sentences. Longer documents are split into consecutive, non-overlapping chunks of 128 sentences and treated as independent documents. \n",
      "...   In essense, the hierarchical nature of this model is close to the recent neural approaches such as (Koshorek et al., 2018). \n",
      "..   Hierarchical BERT \n",
      "...   Our third model is a hierarchical BERT model that also encodes full documents, replacing the document-level LSTM encoder from the BERT+BiLSTM model with a transformer encoder. This architecture is similar to the HIBERT model used for document summarization (Zhang et al., 2019), encoding each sentence independently. The [CLS] token representations from sentences are passed into the document encoder, which is then able to relate the different sentences through cross-attention, as illustrated in Figure 3(c). \n",
      "...   Due to the quadratic computational cost of transformers, we use the same limits as BERT+BiLSTM for input sequence sizes: 64 word-pieces per sentence and 128 sentences per document. \n",
      "...   To keep the number of model parameters comparable with our other proposed models, we use 12 layers for both the sentence and the document encoders, for a total of 24 layers. In order to use the BERTBase checkpoint for these experiments, we use 12 attention heads and 768-dimensional \n",
      "...   word-piece embeddings. \n",
      "...   We study two alternative initialization proce\n",
      "...   dures: \n",
      "...   • initializing both sentence and document en\n",
      "...   coders using BERTBase \n",
      "...   • pre-training all model weights on Wikipedia, using the procedure described in (Zhang et al., 2019), which can be summarized as a ”masked sentence” prediction objective, analogously to the ”masked token” pre-training objective from BERT. \n",
      "...   We call this model hierarchical BERT for consistency with the literature. \n",
      ".   Evaluation methodology \n",
      "..   Datasets \n",
      "...   We perform our experiments on datasets commonly used in the literature. Document segmentation experiments are done on Wiki-727K and Choi, while discourse segmentation experiments are done on the RST-DT dataset. We summarize statistics about the datasets in Table 1. \n",
      "...   Wiki-727K The Wiki-727K dataset (Koshorek et al., 2018) contains 727 thousand articles from a snapshot of the English Wikipedia, which are randomly partitioned into train, development and test sets. We re-use the original splits provided by the authors. While several segmentation granularities are possible, the dataset is used to predict section boundaries. The average number of segments per document is 3.5, with an average segment length of 13.6 sentences. \n",
      "...   We found that the preprocessing methodology used on the Wiki-727K dataset can have a notice\n",
      "...   able effect on the ﬁnal numerical results, in particular when ﬁltering lists, code snippets and other special elements. We used the original preprocessing script (Koshorek et al., 2018) for a fair comparison. \n",
      "...   Choi Choi’s dataset (Choi, 2000) is an early dataset containing 700 synthetic documents made of concatenated extracts of news articles. Each document is made of 10 segments, where each segment was created by sampling a document from the Brown corpus and then sampling a random segment length up to 11 sentences. \n",
      "...   This dataset was originally used to evaluate unsupervised segmentation algorithms, so it is somewhat ill-designed to evaluate supervised algorithms. We use this dataset as a best-effort attempt to allow comparison with some of the previous literature. However, we had to create our own splits as no standard splits exist: we randomly sampled 200 documents as a test set and 50 documents as a validation set, leaving 450 documents for training, following evaluation from Li et al. (2018). Since the Brown corpus only contains 500 documents, the same documents are sampled over and over, necessarily resulting in data leakage between the different splits. Its use should therefore be discouraged in future research. \n",
      "...   RST-DT We perform experiments on discourse segmentation on the RST Discourse Treebank (RST-DT) (Carlson et al., 2001). The dataset is composed of 385 Wall Street Journal articles that are part of the Penn Treebank (Marcus et al., 1994), and is split into the train set composed of 347 articles and the test set composed of 38 articles. We found that the choice of a validation set (held out from the train set) has a large impact on model performance. For this reason, we conduct 10-fold cross validation and report the average over test set metrics. \n",
      "...   Since this dataset is used for discourse segmentation, all the segmentation decisions are made at the intra-sentence level (i.e., the context that is used in the decisions is just a sentence). In order to make the evaluation consistent with other systems from the literature we decided to use the sentence splits that are available in the dataset, even though they are not human annotate. For this reason, there are cases in which some EDUs (which were manually annotated) overlap between two sentences. In such cases, we merge the two sentences. \n",
      "...   Docs \n",
      "...   Sections \n",
      "...   Sentences \n",
      "...   Wiki-727K Train Wiki-727K Dev Wiki-727K Test \n",
      "...   582,146 72,354 73,233 \n",
      "...   2,025,358 179,676 182,563 \n",
      "...   26,988,063 3,375,081 3,457,771 \n",
      "...   Choi Train Choi Dev Choi Test \n",
      "...   450 50 200 \n",
      "...   4,500 500 2,000 \n",
      "...   Docs \n",
      "...   Sentences \n",
      "...   RST-DT Train RST-DT Test \n",
      "...   347 38 \n",
      "...   7,028 864 \n",
      "...   31,075 3,291 14,039 \n",
      "...   EDUs \n",
      "...   19,443 2,346 \n",
      "...   Table 1: Statistics about the datasets. \n",
      "..   Metrics \n",
      "...   Following the trend of many studies on text segmentation (Soricut and Marcu, 2003; Li et al., 2018), we evaluate our approaches using Precision, Recall and F1-score with regard to the internal boundaries of the segments only. In our evaluation we do not include the last boundary of each sentence/document, because it would be trivial to categorize it as a positive boundary, which would lead to an artiﬁcial inﬂation of the results. \n",
      "...   To allow comparison with the existing literature, we also use the Pk metric (Beeferman et al., 1999) to evaluate our results on the Choi’s dataset (note that lower Pk scores indicate better performance). k is set, as is customary, to half the average segment size over the reference segmentation. The Pk metric is less harsh than the F1-score in that it takes into account near misses. It is important to note that Pk metric is known to suffer from biases, for example penalizing false negatives more than false positives and discounting errors close to the document extremities (Pevzner and Hearst, 2002). \n",
      ".   Results \n",
      "..   In Table 2, we report results from the document and discourse segmentation experiments on the three datasets presented in Section 4.1. We include several state-of-the-art baselines which had not been compared against one another before, as they have been proposed independently over a short time period: hierarchical Bi-LSTM (Koshorek et al., 2018), SEGBOT (Li et al., 2018) and BiLSTM+CRF+ELMO (Wang et al., 2018). We also include the human annotation baseline from (Wang et al., 2018), providing an additional reference point on the RST-DT dataset to the trained models. We estimate standard deviations for our proposed models and were able to calculate them from \n",
      "..   Precision \n",
      "..   Wiki-727K Recall \n",
      "..   F1 \n",
      "..   Precision \n",
      "..   Bi-LSTM (Koshorek et al., 2018) SEGBOT (Li et al., 2018) Bi-LSTM+CRF (Wang et al., 2018) \n",
      "..   69.3±0.1 49.5±0.2 57.7±0.1 \n",
      "..   \n",
      "..   \n",
      "..   91.6 92.8 \n",
      "..   RST-DT Recall \n",
      "..   92.8 95.7 \n",
      "..   F1 \n",
      "..   92.2 94.3 \n",
      "..   Choi \n",
      "..   Pk \n",
      "..   0.33 \n",
      "..   F1 \n",
      "..   \n",
      "..   Cross-segment BERT 128-128 BERT+Bi-LSTM Hier. BERT \n",
      "..   69.1±0.1 63.2±0.2 66.0±0.1 92.1±0.8 98.0±0.4 95.0±0.5 99.9±0.1 0.07±0.04 67.3±0.1 53.9±0.1 59.9±0.1 94.4±0.5 96.0±0.4 95.2±0.3 99.8±0.1 0.17±0.06 69.8±0.1 63.5±0.1 66.5±0.1 93.8±0.7 96.7±0.5 95.2±0.4 99.5±0.1 0.38±0.09 \n",
      "..   Human (Wang et al., 2018) \n",
      "..   \n",
      "..   \n",
      "..   \n",
      "..   98.3 \n",
      "..   98.2 \n",
      "..   98.5 \n",
      "..   \n",
      "..   \n",
      "..   Table 2: Test set results on text segmentation and discourse segmentation for baselines and our models. Where possible, we estimate standard deviations by bootstrapping the test set 100 times. \n",
      "..   the hierarchical Bi-LSTM, whose code and trained checkpoint were publicly released. \n",
      "..   To train our models, we used the AdamW optimizer (Loshchilov and Hutter, 2017) with a 10% dropout rate as well as a linear warmup procedure. Learning rates are set between 1e-5 and 5e-6, chosen to maximize the F1-score on the validation sets from each dataset. For the more expensive models, and especially on the Wiki-727K dataset, we trained our models using Google Cloud TPUs. \n",
      "..   We can see from the table that our models outperform the baselines across all datasets, reducing the relative error margins from the best baseline by 20%, 16% and 79% respectively on the Wiki-727K, RST-DT and Choi datasets. The improvements are statistically signiﬁcant for all datasets. The errors are impressively low on the Choi dataset, but it is important to point out that it is a small-scale synthetic dataset, and as such limited. Since each document is a concatenation of extracts from random news articles, it is an artiﬁcially easy task for which a previous neural baseline achieved an already low error margin. Moreover, on this dataset, the crosssegment BERT model obtains very good results compared to the hierarchical models which do not attend across the candidate break. This aligns with the expectation that locally attending across a segment break is sufﬁcient here, as we expect large semantic shifts due to the artiﬁcial nature of the dataset. \n",
      "..   Hierarchical models, with a sentence encoder followed by a document encoder, perform well on the RST-DT dataset. As a reminder, this discourse segmentation task is about segmenting individual sentences so there is no notion of document context. In order to study whether the hierarchical structure is really necessary for discourse segmentation, we also trained a model without the Bi-LSTM (that is, making predictions directly using BERT): this \n",
      "..   decreased the F1-score by 0.4%. It is also worth noting that several known LSTM downsides were particularly apparent on the Wiki-727K dataset: the model was harder to train and signiﬁcantly slower during both training and inference. \n",
      "..   Regarding the hierarchical BERT model, different initialization methods were used for the two document segmentation datasets. On the Choi dataset, a HIBERT initialization (a model fully pretrained end-to-end for hierarchical BERT, similarly to (Zhang et al., 2019) was necessary to get good results, due the small dataset size. On the contrary, we obtained slightly better results initializing both levels of the hierarchy with BERTBase on the Wiki727K dataset, even though the model took longer to converge. Other initializations, e.g., random for both levels of the hierarchy or BERTBase at the lower level and random at the upper level, gave worse results. \n",
      "..   Perhaps the most surprising result from Table 2 is the good performance of our cross-segment BERT model across all datasets, since it only relies on local context to make predictions. And while the BERT checkpoints were pre-trained using (among other things) the next-sentence prediction task, it was not clear a priori that our cross-segment BERT model would be able to detect much more subtle semantic shifts. To further evaluate the effectiveness of this model, we tried using longer contexts. In particular, we considered using a cross-segment BERT with 255-255 contexts, achieving 67.1 F1, 73.9 recall and 61.5 precision scores. Therefore, we can see that encoding the full document in a hierarchical manner using transformers does not improve over cross-segment BERT on this dataset. This suggests that BERT self-attention mechanism applied across candidate segment breaks, with a limited context, is in this case just as powerful as separately encoding each sentence and then allow\n",
      "..   ing a ﬂow of information across encoded sentences. In the next section we further analyze the impact of context length on the results from the crosssegment BERT model. \n",
      ".   Analyses \n",
      "..   In this section we perform additional analyses and ablation studies to better understand our segmentation models. \n",
      "..   Experiments revolve around the cross-segment BERT model. We choose this model because it has several advantages over its alternatives: \n",
      "..   • It outperforms all baselines previously reported as state-of-the-art, and its results are competitive with the more complex hierarchical approaches we considered. \n",
      "..   • It is conceptually close to the original BERT model (Devlin et al., 2018), whose code is open-source, and is as such simple to implement. \n",
      "..   • It only uses local document context and therefore does not require encoding an entire document to segment a potentially small piece of text of interest. \n",
      "..   One application for text segmentation is in assisting a document writer in composing a document, for example to save them time and effort. The task proposed by Lukasik and Zens (2018), aligned with what industrial applications such as Google Docs Explore provide, was to recommend related entities to a writer in real time. However, text segmentation could also help authors in structuring their document better by suggesting where a section break might be appropriate. Motivated by this application, we next analyze how much context is needed to reliably predict a section break. \n",
      "..   Role of trailing context size \n",
      "...   For the aforementioned application, it would be helpful to use as little trailing (after-the-break) context as possible. This way, we can suggest section breaks sooner. Reducing the context size also speeds up the model (as cost is quadratic in sequence length). To this end, we study the effect of trailing context size, going from 128 word-piece tokens down to 0. For this set of experiments, we held the leading context size ﬁxed at 128 tokens, and tuned BERTBASE with a batch size of 1536 examples and a learning rate of 5e-5. The results for these 128-n experiments are shown in Figure 4. While the results are intuitive, it is not clear \n",
      "...   Figure 4: Analysis of the importance of the right context length (solid red line). Dashed blue line denotes the hierarchical Bi-LSTM baseline encoding the full context (Koshorek et al., 2018). \n",
      "...   whether the performance drops because of smaller trailing context or because of smaller overall context. To answer this, we ran another experiment with 256 tokens on the left and 0 tokens on the right (256-0). With all else being the same, this 256-0 experiment attains F1 score of 20.2. This is much smaller than 64.0 F1 with 128 tokens on each side of the proposed break. Clearly, it is crucial that the model sees both sides of the break. This aligns with the intuition that word distributions before and after a true segment break are typically quite different (Hearst, 1997). However, presenting the model with just the distributions of tokens on either side of the proposed break leads to poor performance: in another experiment, we replaced the running text on either side with a sorted list of 128 most frequent tokens seen in a larger context (256 tokens) on either side, padding as necessary, and tuned BERTBASE with all else the same. This 128128 experiment attains 39.1 F1 score, compared to 64.0 with 128-128 running text on either side. This suggests that high-performing models are doing more than just counting tokens on each side to detect semantic shift. \n",
      "..   Role of Transformer architecture \n",
      "...   The best cross-segment BERT model relies on BERTLarge. While powerful, this model is slow and expensive to run. For large-scale applications such as ofﬂine analysis for web search or online document processing such as Google Docs or Microsoft Ofﬁce, such large models are prohibitively expensive. Table 3 shows the effect of model size on performance. For these experiments, we initialized the training with models pre-trained as in the \n",
      "...   Architecture \n",
      "...   Parameters \n",
      "...   F1 \n",
      "...   L24-H1024-A16 L12-H768-A12 L12-H512-A8 L12-H256-A8 L6-H256-A8 L4-H256-A4 L12-H128-A8 L6-H128-A8 L12-H64-A8 \n",
      "...   336M 66.0 110M 64.0 54M 63.4 17M 62.3 13M 60.2 11M 58.2 6M 59.2 5M 57.9 2.6M 55.5 \n",
      "...   Table 3: Effect of model architecture on Wiki-727K results. \n",
      "...   BERT paper (Devlin et al., 2018). The ﬁrst two experiments are initialized with BERTLARGE and BERTBASE respectively. \n",
      "...   Overall, the larger the model, the better the performance. These experiments also suggest that, in addition to the size, the conﬁguration also matters. A 128-dimensional model with more layers can outperform a 256-dimensional model with fewer layers. While the new state-of-the-art is several standard deviations better than the previous one (as reported in Table 2), this gain came at a steep cost in the model size. This is unsatisfactory, as large size hinders the possibility of using the model at scale and with low latency, which is desirable for this application (Wang et al., 2018). In the next section, we explore smaller models with better performance using model distillation. \n",
      "..   Model distillation \n",
      "...   As can be seen from the previous section, performance degrades quite quickly as smaller and therefore more practical networks are used. An alternative to the pre-training/ﬁne-tuning approach used above is distillation, which is a popular technique to build small networks (Bucila et al., 2006; Hinton et al., 2015). Instead of training directly a small model on the segmentation data with binary labels, we can instead leverage the knowledge learnt by our best network —called in this context the ’teacher’— as follows. First, we record the predictions, or more precisely the output logits, from the teacher model on the full dataset. Then, a small ’student’ model is trained using a combination of a cross-entropy loss with the true labels, and a MSE loss to mimick the teacher logits. The relative weight between the two objectives is treated as a hyperparameter. \n",
      "...   Distillation results are presented in Table 4. We can see that the distilled models perform better than \n",
      "...   Architecture \n",
      "...   Parameters \n",
      "...   F1 \n",
      "...   L4-H256-A4 L6-H128-A8 \n",
      "...   11M 63.0 5M 62.5 \n",
      "...   Table 4: Distillation results on the Wiki-727K dataset. \n",
      "...   models trained directly on the training data without a teacher, increasing F1-scores by over 4 points. We notice that distillation allows much more compact models to signiﬁcantly outperform the previous state-of-the-art. Unfortunately, we cannot directly compare model sizes with (Koshorek et al., 2018) since they rely on a subset of the embeddings from a public word2vec archive that includes over 3M vocabulary items, including phrases, most of which are likely never used by the model. It is however fair to say their hierarchical Bi-LSTM model relies on dozens of millions of embedding parameters (even though these are not ﬁne-tuned during training) as well as several million LSTM parameters. \n",
      ".   Conclusion \n",
      "..   In this paper, we introduce three new model architectures for text segmentation tasks: a crosssegment BERT model that uses only local context around candidate breaks, as well as two hierarchical models, BERT+Bi-LSTM and hierarchical BERT. We evaluated these three models on document and discourse segmentation using three standard datasets, and compared them with other recent neural approaches. Our experiments showed that all of our models improve the current state-of-theart. In particular, we found that a cross-segment BERT model is extremely competitive with hierarchical models which have been the focus of recent research efforts (Chalkidis et al., 2019; Zhang et al., 2019). This is surprising as it suggests that local context is sufﬁcient in many cases. Due to its simplicity, we suggest at least trying it as a baseline when tackling other segmentation problems and datasets. \n",
      "..   Naturally these results do not imply that hierarchical models should be disregarded. We showed they are strong contenders and we are convinced there are applications where local context is not sufﬁcient. We tried several encoders at the upperlevel of the hierarchy. Our experiments suggest that deep transformer encoders are useful for encoding long and complex inputs, e.g., documents for document segmentation applications, while Bi\n",
      "..   LSTMs proved useful for discourse segmentation. Moreover, RNNs in general may also be useful for very long documents as they are able to deal with very long input sequences. \n",
      "..   Finally, we performed ablation studies to better understand the role of context and model size. Consequently, we showed that distillation is an effective technique to build much more compact models to use in practical settings. \n",
      "..   In future work, we plan to further investigate how different techniques apply to the problem of text segmentation, including data augmentation (Wei and Zou, 2019; Lukasik et al., 2020b) and methods for regularization and mitigating labeling noise (Jiang et al., 2020; Lukasik et al., 2020a). \n"
     ]
    }
   ],
   "source": [
    "tree.print_article_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95cc874e-9451-4833-be9e-f0c2a830a59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4069788 15\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.59702456 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5246991 3\n",
      "new depth\n",
      "maxScore: 0.5253911 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26373816 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.47854233 34\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3576181 6\n",
      "new depth\n",
      "maxScore: 0.38332742 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.52998173 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35932565 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42484608 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45005438 3\n",
      "new depth\n",
      "maxScore: 0.4261103 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26529163 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40617996 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.44962567 6\n",
      "new depth\n",
      "maxScore: 0.35585928 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39458948 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4182567 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5241212 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5259196 4\n",
      "new depth\n",
      "maxScore: 0.5092776 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28527594 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43558758 34\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41763213 6\n",
      "new depth\n",
      "maxScore: 0.3238492 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4480529 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34984425 15\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40622765 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.47899228 4\n",
      "new depth\n",
      "maxScore: 0.45076597 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.25575948 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3443054 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3202807 6\n",
      "new depth\n",
      "maxScore: 0.281796 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.411515 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36020112 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40038413 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.47084212 4\n",
      "new depth\n",
      "maxScore: 0.4186484 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.21485806 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37687683 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4082328 6\n",
      "new depth\n",
      "maxScore: 0.29882756 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42194378 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43522424 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.50404096 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4794134 4\n",
      "new depth\n",
      "maxScore: 0.50460744 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33840406 1\n",
      "new depth\n",
      "maxScore: 0.35770512 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40870166 34\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.340074 6\n",
      "new depth\n",
      "maxScore: 0.2659614 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41133076 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5505736 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.6910342 1\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " In particular, multiple neural approaches have been proposed for the task. In one, a sequence labeling algorithm is proposed where each sentence is encoded using a Bi-LSTM over tokens, and then a Bi-LSTM over sentence encodings is used to label each sentence as ending a segment or not (Koshorek et al., 2018). Authors consider a large dataset based on Wikipedia, and report improvements over unsupervised text segmentation methods. In another work, a sequence-to-sequence model is proposed (Li et al., 2018), where the input is encoded using a BiGRU and segment endings are generated using a pointer network (Vinyals et al., 2015). The authors report signiﬁcant improvements over sequence labeling approaches, however on a dataset composed of 700 artiﬁcial documents created by concatenating segments from random articles from the Brown corpus (Choi, 2000). Lastly, Badjatiya et al. (2018) consider an attention-based CNN-Bi-LSTM model and evaluate it on three small-scale datasets. \n",
      "text2:\n",
      " End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3704278 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39230806 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.44422662 4\n",
      "new depth\n",
      "maxScore: 0.4460861 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1382148 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31334287 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30852914 3\n",
      "new depth\n",
      "maxScore: 0.30593172 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43105808 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36952913 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.446961 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.496409 3\n",
      "new depth\n",
      "maxScore: 0.45447004 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19503774 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42661837 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35828257 3\n",
      "new depth\n",
      "maxScore: 0.38841423 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38270828 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2897973 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.301613 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40355915 4\n",
      "new depth\n",
      "maxScore: 0.44115764 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.08061311 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2591657 34\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2442208 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45971024 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.29756367 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.44192106 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5093741 4\n",
      "new depth\n",
      "maxScore: 0.5735761 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16387586 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36025137 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39879394 8\n",
      "new depth\n",
      "maxScore: 0.39557552 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.49062738 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2966489 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34190753 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45618966 4\n",
      "new depth\n",
      "maxScore: 0.48567262 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17982763 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38103032 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30899817 6\n",
      "new depth\n",
      "maxScore: 0.2582396 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43255776 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39383292 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4865361 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.48438013 4\n",
      "new depth\n",
      "maxScore: 0.4901501 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32211533 1\n",
      "new depth\n",
      "maxScore: 0.30086553 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36493015 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33200967 3\n",
      "new depth\n",
      "maxScore: 0.2679305 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30493328 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33186707 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36410356 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34034252 4\n",
      "new depth\n",
      "maxScore: 0.37435302 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1957387 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30475664 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3093416 1\n",
      "new depth\n",
      "maxScore: 0.19100706 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27826715 0\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.14206299 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18918955 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12702444 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11709437 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15700701 30\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16576762 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.039343573 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39592063 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4491793 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.44004482 4\n",
      "new depth\n",
      "maxScore: 0.45224655 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3360789 1\n",
      "new depth\n",
      "maxScore: 0.33112967 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3669355 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3298057 8\n",
      "new depth\n",
      "maxScore: 0.33983898 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.29590994 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28150883 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41144875 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39167023 4\n",
      "new depth\n",
      "maxScore: 0.43949205 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28730878 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3032338 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31014037 3\n",
      "new depth\n",
      "maxScore: 0.26065627 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22678815 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36192963 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42235574 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35232052 4\n",
      "new depth\n",
      "maxScore: 0.34814823 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.25254655 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.29617167 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.29733706 6\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30489758 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32699734 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30321234 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30195847 4\n",
      "new depth\n",
      "maxScore: 0.31319 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3459343 1\n",
      "new depth\n",
      "maxScore: 0.39781117 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3072918 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3436504 6\n",
      "new depth\n",
      "maxScore: 0.2855177 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27094084 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34581414 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42039788 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40620023 4\n",
      "new depth\n",
      "maxScore: 0.4849247 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2927022 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32063568 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34803563 8\n",
      "new depth\n",
      "maxScore: 0.33569777 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32427546 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18119405 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.054912828 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.057627074 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.14702852 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12654902 30\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16401803 6\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.044023283 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16000253 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.06850753 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0040818313 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.06914434 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16382368 30\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.04994883 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.004361283 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.23072965 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10152827 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15691341 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.020718362 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16770792 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2502816 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0891196 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.21360224 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22483513 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3230111 4\n",
      "new depth\n",
      "maxScore: 0.35185546 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15670174 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33034232 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3459917 1\n",
      "new depth\n",
      "maxScore: 0.1698021 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.21763527 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10628726 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09651098 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11621997 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.036048487 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13307646 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10362548 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.04765322 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20664445 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09840006 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.074844345 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.028713377 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11173107 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12908188 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16516668 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32168373 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36227185 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43910736 4\n",
      "new depth\n",
      "maxScore: 0.50592256 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.25033605 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28170133 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37649572 7\n",
      "new depth\n",
      "maxScore: 0.24336314 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3103099 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2412068 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16492942 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26272106 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.054368764 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2311368 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16712545 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26270872 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31168243 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.55796015 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.49402052 4\n",
      "new depth\n",
      "maxScore: 0.5307958 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22202766 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37975404 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34966493 6\n",
      "new depth\n",
      "maxScore: 0.26420766 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3495033 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2075372 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38536093 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38865075 4\n",
      "new depth\n",
      "maxScore: 0.45452785 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13429603 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33516112 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28460115 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3428808 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28174746 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4271778 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3859923 4\n",
      "new depth\n",
      "maxScore: 0.39330754 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1809094 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3717977 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33532506 6\n",
      "new depth\n",
      "maxScore: 0.24212489 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34420282 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34124452 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41989845 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45840913 4\n",
      "new depth\n",
      "maxScore: 0.48107058 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13806552 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34462023 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32108194 8\n",
      "new depth\n",
      "maxScore: 0.32048017 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.47473204 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09416335 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.119405106 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18096405 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.101059034 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18441768 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09548423 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18142693 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12830298 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22302988 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.021430824 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15835463 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1838136 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2650043 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45456192 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4980775 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5437685 4\n",
      "new depth\n",
      "maxScore: 0.5062061 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22246289 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4166817 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3836298 3\n",
      "new depth\n",
      "maxScore: 0.4172845 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37614536 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28297865 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.290579 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34286916 4\n",
      "new depth\n",
      "maxScore: 0.40068027 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2472029 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27016956 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36344782 7\n",
      "new depth\n",
      "maxScore: 0.25836408 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.255899 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.115916744 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.038396806 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.08583262 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.08288948 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10703307 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.03481351 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22234368 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17981932 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2289049 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16593991 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20464313 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.24120289 6\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13754919 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35580316 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.44125557 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.510147 4\n",
      "new depth\n",
      "maxScore: 0.47375056 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22843552 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40542734 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3772511 6\n",
      "new depth\n",
      "maxScore: 0.332553 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41533622 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.14662015 12\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0072968947 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.045780264 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13696152 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11570804 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.066726536 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10765687 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.055917744 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.08544482 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.029497845 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0817837 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.05401759 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17536697 17\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1160857 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13364811 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0811006 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16509563 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1491801 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12577634 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40972576 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4387701 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34896576 3\n",
      "new depth\n",
      "maxScore: 0.34947157 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20238453 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3330747 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36119017 7\n",
      "new depth\n",
      "maxScore: 0.40670806 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2839582 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35861054 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.55043125 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4897928 4\n",
      "new depth\n",
      "maxScore: 0.47654462 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.21505493 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42501768 38\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38001114 6\n",
      "new depth\n",
      "maxScore: 0.31518 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43483233 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1772183 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09607057 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18110907 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11646116 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12683147 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2276731 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.08850527 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28145185 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34886408 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32264817 3\n",
      "new depth\n",
      "maxScore: 0.37334982 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.24247536 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3895889 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39824674 6\n",
      "new depth\n",
      "maxScore: 0.3434429 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32208127 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3047893 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.53819764 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4248109 4\n",
      "new depth\n",
      "maxScore: 0.42200544 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.23045218 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3512593 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36276573 6\n",
      "new depth\n",
      "maxScore: 0.3089553 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34244934 2\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5049975 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.63790935 2\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. \n",
      "text2:\n",
      " length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3018326 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.50458527 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37099358 4\n",
      "new depth\n",
      "maxScore: 0.34980607 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.23372936 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.25818506 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32657337 6\n",
      "new depth\n",
      "maxScore: 0.28668493 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30909604 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45802408 15\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5735674 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.55178547 4\n",
      "new depth\n",
      "maxScore: 0.55605996 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38198358 1\n",
      "new depth\n",
      "maxScore: 0.3687051 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.51451266 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4316192 1\n",
      "new depth\n",
      "maxScore: 0.3699993 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.46756384 0\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30319473 17\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31460935 6\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3667342 4\n",
      "new depth\n",
      "maxScore: 0.4774107 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31104994 0\n",
      "new depth\n",
      "maxScore: 0.26903492 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.49022815 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42804903 7\n",
      "new depth\n",
      "maxScore: 0.42470193 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32518643 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.54308033 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.6121048 1\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " In particular, multiple neural approaches have been proposed for the task. In one, a sequence labeling algorithm is proposed where each sentence is encoded using a Bi-LSTM over tokens, and then a Bi-LSTM over sentence encodings is used to label each sentence as ending a segment or not (Koshorek et al., 2018). Authors consider a large dataset based on Wikipedia, and report improvements over unsupervised text segmentation methods. In another work, a sequence-to-sequence model is proposed (Li et al., 2018), where the input is encoded using a BiGRU and segment endings are generated using a pointer network (Vinyals et al., 2015). The authors report signiﬁcant improvements over sequence labeling approaches, however on a dataset composed of 700 artiﬁcial documents created by concatenating segments from random articles from the Brown corpus (Choi, 2000). Lastly, Badjatiya et al. (2018) consider an attention-based CNN-Bi-LSTM model and evaluate it on three small-scale datasets. \n",
      "text2:\n",
      " We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18858561 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.24071567 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2026219 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15853488 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3733521 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32018048 7\n",
      "new depth\n",
      "maxScore: 0.30264306 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15574218 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3335644 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2540167 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20656198 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17073318 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4923109 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3758531 8\n",
      "new depth\n",
      "maxScore: 0.38017002 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18013613 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20759839 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20160691 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15896411 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15112075 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39669687 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33749783 8\n",
      "new depth\n",
      "maxScore: 0.33111644 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17039962 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2906967 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3165072 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.33344406 2\n",
      "new depth\n",
      "maxScore: 0.34692228 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.25297368 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2862612 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38063782 8\n",
      "new depth\n",
      "maxScore: 0.38106233 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3891664 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37519783 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3440258 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35084462 4\n",
      "new depth\n",
      "maxScore: 0.31785303 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26098776 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34536886 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32562327 7\n",
      "new depth\n",
      "maxScore: 0.38203657 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30408424 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27609187 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28317538 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3480326 4\n",
      "new depth\n",
      "maxScore: 0.3140402 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17427506 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34241807 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3053388 7\n",
      "new depth\n",
      "maxScore: 0.261937 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3457319 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15887547 12\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10051505 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0041761133 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.065951034 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41352254 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11203434 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0 0\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18048255 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11659795 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.06071504 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10489854 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38764542 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11572781 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.056900214 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22058676 12\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1587371 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11437358 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12821661 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45278922 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19591424 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0765684 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27538544 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3285759 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40804836 4\n",
      "new depth\n",
      "maxScore: 0.45092273 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.183818 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3368088 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.30196387 6\n",
      "new depth\n",
      "maxScore: 0.29721218 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43913227 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18542245 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16591579 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19240658 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.24380179 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28112793 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37874222 8\n",
      "new depth\n",
      "maxScore: 0.36632243 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3500117 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3894419 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.47834897 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4734579 4\n",
      "new depth\n",
      "maxScore: 0.4439649 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31616512 0\n",
      "new depth\n",
      "maxScore: 0.26194814 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42186853 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42505768 7\n",
      "new depth\n",
      "maxScore: 0.49085176 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.37962586 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4341408 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.6030342 2\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. \n",
      "text2:\n",
      " On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27198768 17\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3069137 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3864153 4\n",
      "new depth\n",
      "maxScore: 0.4872291 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.31173378 0\n",
      "new depth\n",
      "maxScore: 0.28752756 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42945462 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38445026 7\n",
      "new depth\n",
      "maxScore: 0.32992655 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.27762592 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2583993 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.51592207 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36204064 4\n",
      "new depth\n",
      "maxScore: 0.34150952 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26018062 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45706207 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42273146 7\n",
      "new depth\n",
      "maxScore: 0.33814228 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20288481 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39705145 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.6206328 3\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. \n",
      "text2:\n",
      " To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.071973324 12\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16065839 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.07263099 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.11217199 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18688086 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20165431 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.0690859 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3867097 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.65338403 4\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the \n",
      "text2:\n",
      " Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.15276304 12\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17978293 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.06912371 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17814283 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.28592753 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19180083 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.079674914 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.23903203 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.24338298 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09287006 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.103036225 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34447038 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2504021 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13307855 3\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1766118 12\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18843284 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10235179 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.09689869 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34240222 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.091491394 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.05503687 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16722696 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.14132878 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1899595 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2371472 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3573351 31\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.26477864 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.14934976 4\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.23488723 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.38933358 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3207091 4\n",
      "new depth\n",
      "maxScore: 0.41836426 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3206554 1\n",
      "new depth\n",
      "maxScore: 0.34214142 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3407783 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.35782444 6\n",
      "new depth\n",
      "maxScore: 0.26319844 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18903458 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4248465 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4754179 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42209578 4\n",
      "new depth\n",
      "maxScore: 0.49018624 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.29999748 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39240822 33\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.43014315 6\n",
      "new depth\n",
      "maxScore: 0.41422248 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3362878 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.57646906 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.63649285 7\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. \n",
      "text2:\n",
      " To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.50257933 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.6170967 2\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. \n",
      "text2:\n",
      " We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3792302 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.67439145 4\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. \n",
      "text2:\n",
      " We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4624927 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.7161692 10\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. \n",
      "text2:\n",
      " Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3549054 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5167521 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3137476 4\n",
      "new depth\n",
      "maxScore: 0.34671307 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.22704706 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36394376 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2910591 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3352286 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18250157 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40766597 6\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18681335 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.008659057 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.1884225 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.12621734 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2939636 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18224695 9\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16769668 7\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.10036577 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17657176 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3585174 20\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.21012571 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.033929452 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18336338 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40428603 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17866436 3\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.13003726 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.21717638 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2593133 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19510022 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.47073826 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.58735317 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5337149 4\n",
      "new depth\n",
      "maxScore: 0.5813214 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34439778 1\n",
      "new depth\n",
      "maxScore: 0.28565484 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5342119 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.48766488 6\n",
      "new depth\n",
      "maxScore: 0.4889285 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4238064 0\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.53129804 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.6689372 10\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. \n",
      "text2:\n",
      " In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.39109066 15\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.59523237 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.574665 4\n",
      "new depth\n",
      "maxScore: 0.5764438 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19405603 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.42243484 34\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.32979056 1\n",
      "new depth\n",
      "maxScore: 0.27853304 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.45491037 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5245769 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.79149735 4\n",
      "INFO: A node inserted.\n",
      "text1:\n",
      " On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. \n",
      "text2:\n",
      " For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. \n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.36009228 11\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.55259603 10\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.5167272 4\n",
      "new depth\n",
      "maxScore: 0.49815667 2\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.24377576 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.40191585 37\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.34811798 6\n",
      "new depth\n",
      "maxScore: 0.28436935 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4904596 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2733413 15\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.3173632 16\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.29227775 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.124724045 0\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.41003048 32\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.4270566 3\n",
      "new depth\n",
      "maxScore: 0.28762263 8\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.2114654 1\n",
      "new node\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20616084 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.20716006 15\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.16166282 4\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.18147406 1\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19332437 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.17666566 5\n",
      "new title\n",
      "new depth\n",
      "maxScore: 0.19128747 2\n",
      "Kodun çalışma süresi: 547.6457123756409 saniye\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "# Kodun çalışma süresini ölçmek istediğiniz bloğu buraya yerleştirin\n",
    "start_time = time.time()\n",
    "\n",
    "output = tree.merge_trees(tree2,0.6,False)\n",
    "end_time = time.time()\n",
    "\n",
    "# Kodun çalışma süresini hesapla ve yazdır\n",
    "execution_time = end_time - start_time\n",
    "print(\"Kodun çalışma süresi:\", execution_time, \"saniye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203631b7-6fbd-405c-b2ca-e693410a23c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = tree.summarize_parts(0.4, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16b5db-de23-4c69-a063-972e117fff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715ca378-09df-4326-bc1f-8f1cb17e521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sim yüklendi: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from Modules.load_models import load_model_similarty\n",
    "load_model_similarty()\n",
    "from Modules.similarity import calculate_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e4316d-e510-4929-b207-841ab318e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 =\"In particular, multiple neural approaches have been proposed for the task. In one, a sequence labeling algorithm is proposed where each sentence is encoded using a Bi-LSTM over tokens, and then a Bi-LSTM over sentence encodings is used to label each sentence as ending a segment or not (Koshorek et al., 2018). Authors consider a large dataset based on Wikipedia, and report improvements over unsupervised text segmentation methods. In another work, a sequence-to-sequence model is proposed (Li et al., 2018), where the input is encoded using a BiGRU and segment endings are generated using a pointer network (Vinyals et al., 2015). The authors report signiﬁcant improvements over sequence labeling approaches, however on a dataset composed of 700 artiﬁcial documents created by concatenating segments from random articles from the Brown corpus (Choi, 200\"\n",
    "str11 = \"Document segmentation Many early research efforts were focused on unsupervised text segmentation, doing so by quantifying lexical cohesion within small text segments (Hearst, 1997; Choi, 2000). Being hard to precisely deﬁne and quantify, lexical cohesion has often been approximated by counting word repetitions. Although computationally expensive, unsupervised Bayesian approaches have also been popular (Utiyama and Isahara, 2001; Eisenstein, 2009; Mota et al., 2019). However, unsupervised algorithms suffer from two main drawbacks: they are hard to specialize for a given domain and in most cases do not naturally deal with multi-scale issues. Indeed, the desired segmentation granularity (paragraph, section, chapter, etc.) is necessarily task dependent and supervised learning provides a way of addressing this property. Therefore, supervised algorithms have been a focus of many recent works. \"\n",
    "\n",
    "\n",
    "str2 = \"What is Document segmantation.\"\n",
    "\n",
    "str2 = \"End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. \"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d091d374-0ad4-4e8e-bdeb-3d3afc05f36f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'str1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m calculate_similarity([\u001b[43mstr1\u001b[49m,str2])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'str1' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_similarity([str1,str2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4e550c-06d8-4208-931f-d75edfb228e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metinler arasındaki benzerlik: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similarity_tf_idf(text1, text2):\n",
    "    # TF-IDF vektörlerini oluşturma\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # Kosinüs benzerliğini hesaplama\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    return cosine_sim[0][0]\n",
    "\n",
    "# Örnek kullanım\n",
    "text1 = \"Bu bir örnek cümle.\"\n",
    "text2 = \"Bu da başka bir örnek cümle.\"\n",
    "similarity_score = find_similarity_tf_idf(text1, \"Scaled Dot-Product Attention\")\n",
    "print(\"Metinler arasındaki benzerlik:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb6fe1e-fa37-4c89-86d9-feec0790f710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. ',\n",
       "  0.5020684171078611,\n",
       "  0.7660131),\n",
       " ('The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor . Additive attention computes the compatibility function using a feed-forward network with of a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. ',\n",
       "  0.3564228047618501,\n",
       "  0.6959162),\n",
       " ('We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values. ',\n",
       "  0.2155508994618224,\n",
       "  0.73709387),\n",
       " ('While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ dk ',\n",
       "  0.17997533970312757,\n",
       "  0.69542336)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = tree2.get_context(\"Scaled Dot-Product Attention\",5)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c02029e5-395c-4068-bd43-50b477262756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'variables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1 = \"Scaled Dot-Product Attention\"\n",
    "str2 = arr[4][0]\n",
    "str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "601423fb-166e-49c8-8db6-0371fb2399eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3499761"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_similarity([str1,str2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44487e-4eab-478a-9c34-26fd9195fa49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
