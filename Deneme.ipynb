{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb54fca-9fc5-4629-809b-f85f02c14c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Dosyayı aç ve pickle.dump ile nesneyi yaz\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Dosyayı aç ve pickle.dump ile nesneyi yaz\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60715b42-f319-483d-9547-d95e60bec52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Eğer henüz üstteki modeller yüklenmediyse bunu yukarı kaydırıp onları yükle\n",
    "\"\"\"\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline, AutoTokenizer, BartForConditionalGeneration\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a776fa5-c4dc-41ad-8efe-f422c1f7f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Dosyayı aç ve pickle.load ile nesneyi oku\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eb3ebbd-e07b-4566-8b5b-90e3f231a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title A High-Performance Deep Neural Network Model for BI-RADS Classification of Screening Mammography \n",
      "girdi2 <LTTextBoxHorizontal(16) 166.394,189.847,234.508,199.810 '1. Introduction\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(9) 166.395,159.671,327.201,169.634 '2. Materials and Lesion Annotation\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(11) 166.394,187.908,290.428,197.871 '3. Methodology and Model\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(4) 166.394,294.498,307.464,304.461 '3.1. Block Images as Training Data\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(21) 166.394,193.770,262.742,203.733 '3.2. Model Architecture\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(10) 166.394,267.682,274.060,277.645 '4. Experimental Results\\n'>\n",
      "girdi2 <LTTextBoxHorizontal(2) 166.394,734.418,233.393,744.381 '5. Conclusions\\n'>\n",
      "References is detected.\n",
      "   4076\n",
      ".   782\n",
      "..   130\n",
      "..   2\n",
      "(cid:1)(cid:2)(cid:3)(cid:1)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7) \n",
      "..   6\n",
      "Citation: Tsai, K.-J.; Chou, M.-C.; Li, \n",
      "..   6\n",
      "H.-M.; Liu, S.-T.; Hsu, J.-H.; Yeh, \n",
      "..   5\n",
      "W.-C.; Hung, C.-M.; Yeh, C.-Y.; \n",
      "..   4\n",
      "Hwang, S.-H. A High-Performance \n",
      "..   5\n",
      "Deep Neural Network Model for \n",
      "..   4\n",
      "BI-RADS Classiﬁcation of Screening \n",
      "..   4\n",
      "Mammography. Sensors 2022, 22, \n",
      "..   2\n",
      "1160. https://doi.org/10.3390/ \n",
      "..   1\n",
      "s22031160 \n",
      "..   5\n",
      "Academic Editors: Jian Cao, Chintan \n",
      "..   5\n",
      "Bhatt, Monowar H. Bhuyan, Behnaz \n",
      "..   4\n",
      "Ghoraani and Mukesh Prasad \n",
      "..   4\n",
      "Received: 5 January 2022 \n",
      "..   4\n",
      "Accepted: 31 January 2022 \n",
      "..   4\n",
      "Published: 3 February 2022 \n",
      "..   5\n",
      "Publisher’s Note: MDPI stays neutral \n",
      "..   6\n",
      "with regard to jurisdictional claims in \n",
      "..   5\n",
      "published maps and institutional afﬁl\n",
      "..   1\n",
      "iations. \n",
      "..   6\n",
      "Copyright: © 2022 by the authors. \n",
      "..   4\n",
      "Licensee MDPI, Basel, Switzerland. \n",
      "..   7\n",
      "..   2\n",
      "distributed under \n",
      "..   3\n",
      "the terms and \n",
      "..   5\n",
      "conditions of the Creative Commons \n",
      "..   5\n",
      "Attribution (CC BY) license (https:// \n",
      "..   1\n",
      "creativecommons.org/licenses/by/ \n",
      "..   1\n",
      "4.0/). \n",
      "..   5\n",
      "Sensors 2022, 22, 1160. https://doi.org/10.3390/s22031160 \n",
      "..   1\n",
      "https://www.mdpi.com/journal/sensors \n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "2 of 15 \n",
      "..   42\n",
      "..   74\n",
      "..   105\n",
      "..   83\n",
      "..   90\n",
      "..   84\n",
      "..   45\n",
      ".   467\n",
      "..   40\n",
      "..   42\n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "3 of 15 \n",
      "..   40\n",
      "..   10\n",
      "..   1\n",
      "BI-RADS \n",
      "..   1\n",
      "Deﬁnition \n",
      "..   1\n",
      "Management \n",
      "..   3\n",
      "Likelihood of Cancer \n",
      "..   1\n",
      "0 \n",
      "..   1\n",
      "1 \n",
      "..   1\n",
      "2 \n",
      "..   1\n",
      "3 \n",
      "..   1\n",
      "4A \n",
      "..   1\n",
      "4B \n",
      "..   1\n",
      "4C \n",
      "..   1\n",
      "5 \n",
      "..   1\n",
      "6 \n",
      "..   5\n",
      "Incomplete, need additional imaging evaluation \n",
      "..   8\n",
      "..   2\n",
      "Negative (normal) \n",
      "..   2\n",
      "Routine screening \n",
      "..   1\n",
      "Benign \n",
      "..   2\n",
      "Routine screening \n",
      "..   1\n",
      "– \n",
      "..   1\n",
      "0% \n",
      "..   1\n",
      "0% \n",
      "..   2\n",
      "Probably benign \n",
      "..   4\n",
      "Short-interval follow-up or continued \n",
      "..   3\n",
      "≤ ≤ ≤ \n",
      "..   3\n",
      ">0% to ≤2% \n",
      "..   3\n",
      ">2% to ≤10% \n",
      "..   4\n",
      "Low suspicion of malignancy \n",
      "..   2\n",
      "Tissue diagnosis \n",
      "..   4\n",
      "Moderate suspicion of malignancy \n",
      "..   4\n",
      "High suspicion of malignancy \n",
      "..   4\n",
      "Highly suggestive of malignancy \n",
      "..   3\n",
      "Known biopsy-proven malignancy \n",
      "..   2\n",
      "Tissue diagnosis \n",
      "..   3\n",
      ">10% to ≤50% \n",
      "..   1\n",
      "≥ \n",
      "..   2\n",
      "Tissue diagnosis \n",
      "..   3\n",
      ">50% to <95% \n",
      "..   2\n",
      "Tissue diagnosis \n",
      "..   5\n",
      "Surgical excision when clinically appropriate \n",
      "..   1\n",
      "≥95% \n",
      "..   1\n",
      "100% \n",
      "..   95\n",
      "..   8\n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "4 of 15 \n",
      "..   1\n",
      "(a)  \n",
      "..   1\n",
      "(b)  \n",
      "..   23\n",
      "..   67\n",
      "..   10\n",
      "..   1\n",
      "BI-RADS \n",
      "..   3\n",
      "Number of Annotations \n",
      "..   9\n",
      "..   9\n",
      ".   1318\n",
      "..   88\n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "5 of 15 \n",
      "..   12\n",
      "..   72\n",
      "..   424\n",
      "...   100\n",
      "...   80\n",
      "...   2\n",
      "ratioB = \n",
      "...   2\n",
      "ratioL = \n",
      "...   4\n",
      "AreaB ∩ AreaL AreaB \n",
      "...   4\n",
      "AreaB ∩ AreaL AreaL \n",
      "...   1\n",
      "(1) \n",
      "...   1\n",
      "(2) \n",
      "...   4\n",
      "Sensors 2022, 22, 1160 \n",
      "...   3\n",
      "6 of 15 \n",
      "...   19\n",
      "...   8\n",
      "...   1\n",
      "(3) \n",
      "...   110\n",
      "...   1\n",
      "(a)  \n",
      "...   1\n",
      "(b)  \n",
      "...   1\n",
      "(c)  \n",
      "...   26\n",
      "...   8\n",
      "...   1\n",
      "BI-RADS \n",
      "...   4\n",
      "Number of Training Data \n",
      "...   4\n",
      "Number of Test Data \n",
      "...   9\n",
      "...   9\n",
      "...   2\n",
      "𝑟𝑎𝑡𝑖𝑜(cid:2886) (cid:3404)   \n",
      "...   4\n",
      "𝐴𝑟𝑒𝑎(cid:2886) ∩ 𝐴𝑟𝑒𝑎(cid:2896) 𝐴𝑟𝑒𝑎(cid:2886) \n",
      "...   2\n",
      "𝑟𝑎𝑡𝑖𝑜(cid:2896) (cid:3404)   \n",
      "...   4\n",
      "𝐴𝑟𝑒𝑎(cid:2886) ∩ 𝐴𝑟𝑒𝑎(cid:2896) 𝐴𝑟𝑒𝑎(cid:2896) \n",
      "...   9\n",
      "..   715\n",
      "...   119\n",
      "...   1\n",
      "≥ \n",
      "...   4\n",
      "Sensors 2022, 22, 1160 \n",
      "...   3\n",
      "7 of 15 \n",
      "...   9\n",
      "...   7\n",
      "...   7\n",
      "...   7\n",
      "...   1\n",
      "(cid:2879)(cid:3081)(cid:3051) \n",
      "...   4\n",
      "𝑥 1 (cid:3397) 𝑒 \n",
      "...   4\n",
      "x 1 + e−βx \n",
      "...   1\n",
      "(4) \n",
      "...   37\n",
      "...   10\n",
      "...   1\n",
      "Module \n",
      "...   2\n",
      "Kernel Size \n",
      "...   1\n",
      "Stride \n",
      "...   2\n",
      "Expansion Ratio \n",
      "...   1\n",
      "Parameters \n",
      "...   2\n",
      "Output Shape \n",
      "...   20\n",
      "...   42\n",
      "...   14\n",
      "...   14\n",
      "...   14\n",
      "...   54\n",
      "...   1\n",
      "− \n",
      "...   1\n",
      "− \n",
      "...   3\n",
      "8 of 15 \n",
      "...   128\n",
      "...   4\n",
      "Sensors 2022, 22, 1160 \n",
      "...   1\n",
      "(a)  \n",
      "...   1\n",
      "(b)  \n",
      "...   13\n",
      "...   4\n",
      "Sensors 2022, 22, 1160 \n",
      "...   3\n",
      "9 of 15 \n",
      "...   114\n",
      "...   7\n",
      "...   45\n",
      "...   4\n",
      "Table 5. Development environment. \n",
      "...   2\n",
      "Programing Language \n",
      "...   1\n",
      "Python \n",
      "...   1\n",
      "Library \n",
      "...   1\n",
      "Hardware \n",
      ".   1059\n",
      "..   5\n",
      "TensorFlow, Keras, numpy, OpenCV, etc. \n",
      "..   17\n",
      "..   43\n",
      "..   80\n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "10 of 15 \n",
      "..   11\n",
      "..   9\n",
      "..   7\n",
      "..   7\n",
      "..   7\n",
      "..   9\n",
      "..   1\n",
      "(5) \n",
      "..   1\n",
      "(6) \n",
      "..   1\n",
      "(7) \n",
      "..   1\n",
      "(8) \n",
      "..   86\n",
      "..   2\n",
      "mean(x) = \n",
      "..   2\n",
      "1 CNum \n",
      "..   3\n",
      "CNum ∑ k=1 \n",
      "..   11\n",
      "..   1\n",
      "(9) \n",
      "..   2\n",
      "Accuracy = \n",
      "..   1\n",
      "∑CNum \n",
      "..   3\n",
      "k=1 TPk TNum \n",
      "..   1\n",
      "= \n",
      "..   1\n",
      "∑CNum \n",
      "..   5\n",
      "k=1 TPk (TPk + FNk) \n",
      "..   2\n",
      "∑CNum k=1 \n",
      "..   1\n",
      "(10) \n",
      "..   9\n",
      "..   71\n",
      "..   1\n",
      "≤ \n",
      "..   1\n",
      "≤ \n",
      "..   2\n",
      "𝑚𝑒𝑎𝑛(cid:4666)𝑥(cid:4667) (cid:3404) \n",
      "..   8\n",
      "..   1\n",
      "(cid:3004)(cid:3015)(cid:3048)(cid:3040) \n",
      "..   1\n",
      "1 \n",
      "..   2\n",
      "  (cid:3533) 𝑥(cid:3038) \n",
      "..   1\n",
      "(cid:3038)(cid:2880)(cid:2869) \n",
      "..   1\n",
      "𝐶𝑁𝑢𝑚 \n",
      "..   2\n",
      "𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 (cid:3404) \n",
      "..   1\n",
      "(cid:3004)(cid:3015)(cid:3048)(cid:3040) \n",
      "..   1\n",
      "(cid:3038)(cid:2880)(cid:2869) \n",
      "..   1\n",
      "∑ \n",
      "..   1\n",
      "𝑇𝑃(cid:3038) \n",
      "..   1\n",
      "𝑇𝑁𝑢𝑚 \n",
      "..   1\n",
      "(cid:3004)(cid:3015)(cid:3048)(cid:3040) \n",
      "..   1\n",
      "(cid:3038)(cid:2880)(cid:2869) \n",
      "..   1\n",
      "(cid:3404) \n",
      "..   3\n",
      "∑ (cid:3004)(cid:3015)(cid:3048)(cid:3040) (cid:3038)(cid:2880)(cid:2869) \n",
      "..   1\n",
      "∑ \n",
      "..   4\n",
      "𝑇𝑃(cid:3038) (cid:4666)𝑇𝑃(cid:3038) (cid:3397) 𝐹𝑁(cid:3038)(cid:4667) \n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "11 of 15 \n",
      "..   30\n",
      "..   8\n",
      "..   8\n",
      "..   1\n",
      "BI-RADS \n",
      "..   2\n",
      "Sensitivity (%) \n",
      "..   2\n",
      "Speciﬁcity (%) \n",
      "..   2\n",
      "Precision (%) \n",
      "..   2\n",
      "F1-Score (%) \n",
      "..   9\n",
      "..   9\n",
      "..   9\n",
      "..   9\n",
      "..   9\n",
      "..   2\n",
      "Accuracy (%) \n",
      "..   1\n",
      "94.2171 \n",
      "..   45\n",
      "..   56\n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "12 of 15 \n",
      "..   17\n",
      "..   8\n",
      "..   73\n",
      "..   65\n",
      "..   43\n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "13 of 15 \n",
      "..   1\n",
      "(a)  \n",
      "..   1\n",
      "(b)  \n",
      "..   1\n",
      "(c)  \n",
      "..   1\n",
      "(d)  \n",
      "..   1\n",
      "(e)  \n",
      "..   1\n",
      "(f)  \n",
      "..   32\n",
      "..   17\n",
      "..   1\n",
      "Task \n",
      "..   2\n",
      "Dataset Used \n",
      "..   2\n",
      "Ave_Sen (%) \n",
      "..   2\n",
      "Ave_Spe (%) \n",
      "..   2\n",
      "Acc (%) \n",
      "..   1\n",
      "AUC \n",
      "..   5\n",
      "Private (1490 cases, 5733 images) \n",
      "..   1\n",
      "95.31 \n",
      "..   1\n",
      "99.15 \n",
      "..   1\n",
      "94.22 \n",
      "..   1\n",
      "0.972 \n",
      "..   13\n",
      "..   8\n",
      "..   5\n",
      "Mass malignancy classiﬁcation (2 classes) \n",
      "..   5\n",
      "Private (384 cases, 824 images) \n",
      "..   1\n",
      "85.3 \n",
      "..   1\n",
      "91.9 \n",
      "..   5\n",
      "DDSM (2578 cases, 10,312 images) \n",
      "..   5\n",
      "Private (2807 cases, 11,228 images) \n",
      "..   9\n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   1\n",
      "0.910 \n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   1\n",
      "84.5 \n",
      "..   1\n",
      "94.25 \n",
      "..   1\n",
      "84.5 \n",
      "..   0\n",
      "\n",
      "..   0\n",
      "\n",
      "..   1\n",
      "91 \n",
      "..   1\n",
      "0.98 \n",
      "..   8\n",
      "..   11\n",
      "..   6\n",
      "1 FPPI: false positive per image. \n",
      "..   3\n",
      "DDSM (500 images) \n",
      "..   5\n",
      "DDSM + CBIS-DDSM (2339 images) \n",
      "..   2\n",
      "Reference (Year) \n",
      "..   3\n",
      "This study (2022) \n",
      "..   2\n",
      "[8] (2021) \n",
      "..   2\n",
      "[11] (2021) \n",
      "..   2\n",
      "[13] (2020) \n",
      "..   2\n",
      "[22] (2019) \n",
      "..   4\n",
      "Sensors 2022, 22, 1160 \n",
      "..   3\n",
      "14 of 15 \n",
      ".   450\n",
      "..   146\n",
      "..   69\n",
      "..   61\n",
      "..   90\n",
      "..   17\n",
      "..   15\n",
      "..   15\n",
      "..   27\n",
      "..   10\n"
     ]
    }
   ],
   "source": [
    "from Modules.article_tree import ArticleTree\n",
    "from Modules.reader import get_tree_from_article_pdf\n",
    "\n",
    "pdf_path = \"1706.03762.pdf\"\n",
    "pdf_path = \"NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf\"\n",
    "pdf_path = \"document_similartiy\\\\article\\\\TCP_Fairness_Among_Modern_TCP_Congestion_Control_Algorithms_Including_TCP_BBR.pdf\"\n",
    "pdf_path = \"A High-Performance Deep Neural Network Model for BI-RADS Classification of Screening Mammography.pdf\"\n",
    "tree = get_tree_from_article_pdf(pdf_path)\n",
    "tree.calculate_count_of_words()\n",
    "tree.print_count_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc874e-9451-4833-be9e-f0c2a830a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree.print_article_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "203631b7-6fbd-405c-b2ca-e693410a23c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummarizeR 1   1\n",
      "What we think that we send token 866\n",
      "What we think that we send token 678\n",
      "SummarizeR 1   2\n",
      "SummarizeR 1   3\n",
      "What we think that we send token 276\n",
      "What we think that we send token 903\n",
      "SummarizeR 2   3.2\n",
      "What we think that we send token 858\n",
      "What we think that we send token 526\n",
      "SummarizeR 1   4\n",
      "What we think that we send token 1009\n",
      "What we think that we send token 1017\n",
      "What we think that we send token 760\n",
      "SummarizeR 1   5\n"
     ]
    }
   ],
   "source": [
    "result = tree.summarize_parts(0.4, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd16b5db-de23-4c69-a063-972e117fff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction \n",
      "Globally, the incidence rate for breast cancer ranks first. Screening mammography has been acknowledged as the most reliable way to detect breast cancer at an early stage. A great number of mammograms are collected in a large-scale mammography screening program. There is deﬁnitely an unmet need to develop AI models to assist radiologists with mammographic interpretation.\n",
      "Deep learning models have been used to measure the likelihood of cancer from a mammogram. This paper presents a deep learning model to address the BI-RADSclassiﬁcation issue. For the ﬁrst time in the literature, breast lesions can be completely classi ﬉ed using a deeplearning model. The tool can assist radiologists with mammographic interpretation in clinical works and can improve the efﬃciency of mammogram interpretation as well.\n",
      "\n",
      "Materials and Lesion Annotation \n",
      "The digital mammogram dataset employed in this work is provided by the E-Da hospital, Taiwan. The dataset is composed of up to 5733 mammograms of 1490 patients, within the time frame    of 2004 and 2010. To facilitate data preprocessing, an easy-to-use tool was exclusively developed for users to label the lesion in each mammogram. Once the image labeling was completed, an interface, as illustrated in Figure 1, appeared to give users detailed annotation.\n",
      "\n",
      "Methodology and Model \n",
      "This paper presents a DNN-based model to classify mammograms into categories 0, 1, 2, 3, 4A, 4B, 4C and 5, but excluding category 6. The model was trained using block-based images segmented from the dataset.\n",
      "The presented model was trained using a multitude of blockbased images of size 224 × 224 pixels in this work. All the block images were divided into two parts, as the training and test data, respectively. Table 3 gives the numbers of these data for each BI-RADS category. The data were used to train and test the mammogram model for breast cancer. The results were published in the journal Breast Cancer Research and are available on the company's website.\n",
      "The model was built based on one of the state-of-the-art models, EfﬁcientNet [24]. As illustrated in Figure 5, the model is made up of a stem, a body, a head and an output mode. The model takes a mammogram of size 224 × 224 pixels as an input. As compared with ReLU, the performance of a neural network can be improved in most cases using a Swish activation function.\n",
      "An MBConv block is mainly composed of an expansion layer, a depthwise layer and a squeeze-andexcitation network (SENet) The values of the parameters Wd, Hd and Co can be referenced in Table 4. A categorical cross-entropy loss function was used to train the model with a batch size of 128 and an epoch of 350.\n",
      "\n",
      "Experimental Results \n",
      "A confusion matrix for an eight-class classiﬁcation system and four performance metrics for each class were evaluated to quantify the model performance. The mean value of each performance metric and the overall accuracy were found. The outperformance of this work was clearly indicated by an average sensitivity of       (cid:4668)   The receiver operating characteristic (ROC) curve was plotted for each BI-RADS category in Figure 10.\n",
      "of 95.31%, an average speciﬁcity of 99.15%, anAverage precision of 94.93%, an Average F1-score of 95.11%, and an average AUC of 97.23%. In each case of BI-RADS category 0, 4A, 4B, 4C and 5 lesions, the sensitivity, speci ﬁ city and precision exceeded 98%, 99% and 96%.\n",
      "The outperformance of this model was indicated by an overall accuracy of 94.22%, an average sensitivity of 95.31% and an average speciﬁcity of 99.15%. There is a good agreement between the red framed ground truth and the blocks, highlighted in color, in each of the mammograms.\n",
      "\n",
      "Conclusions \n",
      "This paper presented a DNN-based model to efﬁciently and reliably locate and classify breast lesions from mammograms. Block-based images, segmented from collected mammograms, were used to adequately train the model. The outperformance of this model was indicated by an overall accuracy of 94.22%, an average sensitivity of 95.31%, and an average speciﬀcity of 99.15%. When applied to breast cancer screening for Asian women, this model is expected to give a higher accuracy than others in the literature.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ca378-09df-4326-bc1f-8f1cb17e521d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
